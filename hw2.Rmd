---
title: "hw2_alzheimers"
author: "Emma Bradley"
date: "2025-10-06"
output: pdf_document
---

Diagnosis:
0 - normal cognition (base)
1 - mild cognitive impairment due to AD
2 - dementia due to AD

Cognitive and behavioral measures: NACCMMSE, MOTSEV, DISNSEV, ANXSEV, NACCGDS
Physiological measures: BPSYS, BPDIAS, HRATE
Demographics: Age, Educ, Female, Height, Weight

```{r}
library(glmnet) 
set.seed(533)
```
1. Modeling Process and Decisions

Next we will build a multinomial logsitic regression model so that we can see how adding in the third diagnosis option affects our results. Let's go ahead and select the same predictors as we used before to build the multinomial logsitic regression model.


```{r}
raw.data = read.csv('alzheimer_data.csv')

keep.cols = c( "diagnosis",
  # cognitive and behavioral measures
  "naccmmse", "motsev", "disnsev", "anxsev", "naccgds",
  # physiological measures
  "bpsys", "bpdias", "hrate",
  # demographics
  "age", "educ", "female", "height", "weight"
)

keep.cols2 = c( "diagnosis", "age", "educ", "weight", "cdrglob", "hallsev", "agitsev", "mealprep", "travel", "naccmmse",
                "memunits", "digif", "animals", "trailb", "csfvol", "frcort", "frcort", "lent", "lposcin", "rposcin")

df = raw.data[, keep.cols2, drop = FALSE]
# 2700 rows
head(df)
```

We will use the package `glmnet` as it has a built in function that allows us to do multinomial logistic regression and the textbook recommends it. The function is simple and straightforward to use. We first specify our matrix of predictors and target variable. We also specify that the family we are using is multinomial. 

```{r}
X = as.matrix(df[, 2:19]) # predictors
Y =  df$diagnosis # target
model =  glmnet(X, Y, family = "multinomial")
summary(model)
plot(model)
```

cross-validation with 5 folds
both cv5 and cv10 performed better with alpha=1 (LASSO)

Comparative Results Across Validation Strategies

Summarize and compare model performance under different cross-validation approaches (LOOCV, single fold, 5-fold, 10-fold).

Provide appropriate tables and/or figures to support your comparison.

Interpretation of Model Coefficients

Explain the meaning of the coefficients from the final selected models, connecting results to the context of Alzheimerâ€™s research.

Discussion of Modeling Approaches

Reflect on the strengths and limitations of linear regression, binary logistic regression, and multinomial logistic regression.

Discuss how these approaches provide complementary insights into the data.

```{r}
cv5  = cv.glmnet(X, Y, family = "multinomial", alpha=1, nfolds = 5)
pred5  = predict(cv5, newx = X, s = "lambda.min", type = "class")
features5 = lapply(coef(cv5, s = "lambda.min"), function(mat) {
  rownames(mat)[which(mat != 0)]})
features5
coef(cv5, s = "lambda.min")
```

Cross-validation with 10 folds
```{r}
cv10 = cv.glmnet(X, Y, family = "multinomial", alpha=1, nfolds = 10)
pred10 = predict(cv10, newx = X, s = "lambda.min", type = "class")
features10 = lapply(coef(cv10, s = "lambda.min"), function(mat) {
  rownames(mat)[which(mat != 0)]})
#features10
coef(cv10, s = "lambda.min")
```

Cross-validation with a single fold (training/test split)
```{r}
# single fold (train/test split)
# 80 / 20
set.seed(533)
train_idx = sample(seq_len(nrow(X)), size = 0.8 * nrow(X)) #
cv1 = glmnet(X[train_idx, ], Y[train_idx], family = "multinomial")
pred = predict(cv1, newx = X[-train_idx, ], s = 0.01, type = "class")
accuracy.1fold = mean(pred == Y[-train_idx])
```
Results
```{r}
results = data.frame(
  Method = c("1-Fold (Train/Test)", "5-Fold", "10-Fold"),
  Accuracy = c(accuracy.1fold, mean(pred5 == Y), mean(pred10 == Y))
)
print(results)
```
same results for both 5 and 10 fold. The models were likely fit similarly because there is plenty of data to fit the models on. So splitting into 5 vs 10 folds didnt make a huge difference
```{r}
cv5$lambda.min
cv10$lambda.min
mean(cv5$cvm)
mean(cv10$cvm)
```

```{r}
plot(cv5, xvar = "lambda", label = TRUE)
plot(cv10, xvar = "lambda", label = TRUE)
```
